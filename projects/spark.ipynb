{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-c3c95c6330c7>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-c3c95c6330c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-c3c95c6330c7>:4 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd = sc.parallelize(list_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: Row(name=x[0], age=int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_p = [('John',19), ('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd = sc.parallelize(list_p)\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "sc.addFile(url)\n",
    "sqlCont = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlCont.read.csv(SparkFiles.get('adult_data.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('age','race').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('education').count().sort('count', ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'educational_num', 'capital_loss', 'hours-per-week']\n",
    "# Convert the type\n",
    "df_string = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "\n",
    "df_string.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|summary|                 x|               age|  workclass|            fnlwgt|   education|   educational-num|marital-status|      occupation|relationship|              race|gender|      capital-gain|     capital-loss|    hours-per-week|native-country|income|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|  count|             48842|             48842|      48842|             48842|       48842|             48842|         48842|           48842|       48842|             48842| 48842|             48842|            48842|             48842|         48842| 48842|\n",
      "|   mean|           24421.5| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|          null|            null|        null|              null|  null|1079.0676262233324|87.50231358257237|40.422382375824085|          null|  null|\n",
      "| stddev|14099.615260708357|13.710509934443525|       null|105604.02542315757|        null| 2.570972755592259|          null|            null|        null|              null|  null| 7452.019057655418| 403.004552124359|12.391444024252312|          null|  null|\n",
      "|    min|                 1|                17|          ?|             12285|        10th|                 1|      Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|                 0|                0|                 1|             ?| <=50K|\n",
      "|    max|             48842|                90|Without-pay|           1490400|Some-college|                16|       Widowed|Transport-moving|        Wife|             White|  Male|             99999|             4356|                99|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital-gain|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655418|\n",
      "|    min|                 0|\n",
      "|    max|             99999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital-gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|age_income|<=50K|>50K|\n",
      "+----------+-----+----+\n",
      "|        17|  595|   0|\n",
      "|        18|  862|   0|\n",
      "|        19| 1050|   3|\n",
      "|        20| 1112|   1|\n",
      "|        21| 1090|   6|\n",
      "|        22| 1161|  17|\n",
      "|        23| 1307|  22|\n",
      "|        24| 1162|  44|\n",
      "|        25| 1119|  76|\n",
      "|        26| 1068|  85|\n",
      "|        27| 1117| 115|\n",
      "|        28| 1101| 179|\n",
      "|        29| 1025| 198|\n",
      "|        30| 1031| 247|\n",
      "|        31| 1050| 275|\n",
      "|        32|  957| 296|\n",
      "|        33| 1045| 290|\n",
      "|        34|  949| 354|\n",
      "|        35|  997| 340|\n",
      "|        36|  948| 400|\n",
      "+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age', 'income').sort('age_income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age>20).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|      marital-status| avg(capital-gain)|\n",
      "+--------------------+------------------+\n",
      "|           Separated| 581.8424836601307|\n",
      "|       Never-married|  384.382639449029|\n",
      "|Married-spouse-ab...| 629.0047770700637|\n",
      "|            Divorced| 793.6755615860094|\n",
      "|             Widowed| 603.6442687747035|\n",
      "|   Married-AF-spouse|2971.6216216216217|\n",
      "|  Married-civ-spouse|1739.7006121810625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital-status').agg({'capital-gain':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "age_square = df.select(col('age')**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('age_square', col('age')**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=25, age_square=625.0, workclass='Private', fnlwgt=226802, education='11th', educational-num=7, marital-status='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', gender='Male', capital-gain=0, capital-loss=0, hours-per-week=40, native-country='United-States', income='<=50K')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt',\n",
    "             'education',\n",
    "             'educational-num',\n",
    "             'marital-status',\n",
    "             'occupation',\n",
    "             'relationship',\n",
    "             'race',\n",
    "             'gender',\n",
    "             'capital-gain',\n",
    "             'capital-loss',\n",
    "             'hours-per-week',\n",
    "             'native-country',\n",
    "             'income']\n",
    "df = df.select(COLUMNS)\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native-country|count(native-country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|                Laos|                   23|\n",
      "|          Yugoslavia|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|                Hong|                   30|\n",
      "|            Thailand|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|              Greece|                   49|\n",
      "|           Nicaragua|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('native-country').agg({'native-country':'count'}).sort(asc('count(native-country)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: integer (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: integer (nullable = true)\n",
      " |-- capital_loss: integer (nullable = true)\n",
      " |-- hours_week: integer (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "oldColumns = df.columns\n",
    "newColumns = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "              'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "              'hours_week', 'native_country', 'label']\n",
    "\n",
    "df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(newColumns)), df)\n",
    "df.printSchema()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df.filter(df.native_country !=\t'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+------+---------+-------------+------------------+-----------------+------------+-----+----+------------+------------+----------+--------------+-----+-----------------+-------------+\n",
      "|age|age_square|workclass|fnlwgt|education|education_num|           marital|       occupation|relationship| race| sex|capital_gain|capital_loss|hours_week|native_country|label|workclass_encoded|workclass_vec|\n",
      "+---+----------+---------+------+---------+-------------+------------------+-----------------+------------+-----+----+------------+------------+----------+--------------+-----+-----------------+-------------+\n",
      "| 25|     625.0|  Private|226802|     11th|            7|     Never-married|Machine-op-inspct|   Own-child|Black|Male|           0|           0|        40| United-States|<=50K|              0.0|(9,[0],[1.0])|\n",
      "| 38|    1444.0|  Private| 89814|  HS-grad|            9|Married-civ-spouse|  Farming-fishing|     Husband|White|Male|           0|           0|        50| United-States|<=50K|              0.0|(9,[0],[1.0])|\n",
      "+---+----------+---------+------+---------+-------------+------------------+-----------------+------------+-----+----+------------+------------+----------+--------------+-----+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndexer = StringIndexer(inputCol = 'workclass', outputCol = 'workclass_encoded')\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol='workclass_encoded', outputCol = 'workclass_vec')\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: integer (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: integer (nullable = true)\n",
      " |-- capital_loss: integer (nullable = true)\n",
      " |-- hours_week: integer (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_remove.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-------------+\n",
      "|       workclass|workclass_encoded|workclass_vec|\n",
      "+----------------+-----------------+-------------+\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|       Local-gov|              2.0|(9,[2],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|               ?|              3.0|(9,[3],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|               ?|              3.0|(9,[3],[1.0])|\n",
      "|Self-emp-not-inc|              1.0|(9,[1],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|     Federal-gov|              6.0|(9,[6],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|               ?|              3.0|(9,[3],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|       State-gov|              4.0|(9,[4],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|               ?|              3.0|(9,[3],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|Self-emp-not-inc|              1.0|(9,[1],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|Self-emp-not-inc|              1.0|(9,[1],[1.0])|\n",
      "|       State-gov|              4.0|(9,[4],[1.0])|\n",
      "|Self-emp-not-inc|              1.0|(9,[1],[1.0])|\n",
      "|Self-emp-not-inc|              1.0|(9,[1],[1.0])|\n",
      "|       Local-gov|              2.0|(9,[2],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|               ?|              3.0|(9,[3],[1.0])|\n",
      "|       Local-gov|              2.0|(9,[2],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|    Self-emp-inc|              5.0|(9,[5],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|       State-gov|              4.0|(9,[4],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|         Private|              0.0|(9,[0],[1.0])|\n",
      "|    Self-emp-inc|              5.0|(9,[5],[1.0])|\n",
      "+----------------+-----------------+-------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.select('workclass', 'workclass_encoded', 'workclass_vec').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'age_square',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_week',\n",
       " 'native_country',\n",
       " 'label']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove.drop('education_num').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# encode all categorical features\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "CONT_FEATURES  = ['age', 'fnlwgt','capital_gain', 'capital_loss', 'hours_week']\n",
    "stages = []\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                    outputCols=[categoricalCol+'classVec'])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# convert label into label indices using StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol='label', outputCol='newlabel')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "assemblerInputs = [c + 'classVec' for c in CATE_FEATURES] + CONT_FEATURES\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [assembler]\n",
    "\n",
    "# create a Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=25, age_square=625.0, workclass='Private', fnlwgt=226802, education='11th', education_num=7, marital='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', sex='Male', capital_gain=0, capital_loss=0, hours_week=40, native_country='United-States', label='<=50K', workclassIndex=0.0, workclassclassVec=SparseVector(8, {0: 1.0}), educationIndex=5.0, educationclassVec=SparseVector(15, {5: 1.0}), maritalIndex=1.0, maritalclassVec=SparseVector(6, {1: 1.0}), occupationIndex=6.0, occupationclassVec=SparseVector(14, {6: 1.0}), relationshipIndex=2.0, relationshipclassVec=SparseVector(5, {2: 1.0}), raceIndex=1.0, raceclassVec=SparseVector(4, {1: 1.0}), sexIndex=0.0, sexclassVec=SparseVector(1, {0: 1.0}), native_countryIndex=0.0, native_countryclassVec=SparseVector(40, {0: 1.0}), newlabel=0.0, features=SparseVector(98, {0: 1.0, 13: 1.0, 24: 1.0, 35: 1.0, 45: 1.0, 49: 1.0, 52: 1.0, 53: 1.0, 93: 25.0, 94: 226802.0, 97: 40.0}))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x['newlabel'], DenseVector(x['features'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sqlCont.createDataFrame(input_data, ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       29675|\n",
      "|  1.0|        9300|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby('label').agg({'label':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.08291539859596332,-0.13128066168454205,-0.04252824549956917,-0.17214090867403273,-0.11991371324225043,0.18083586244548436,0.18694646950356447,-0.2290196886156488,-0.2046951680603273,-0.07381112960044971,0.24417859780038137,0.4251011428795726,-0.005025027787783692,-0.32244065352932066,-0.002005553788510388,-0.35369017203669645,-0.45656053992458595,0.5837186488197419,-0.39515581561627877,-0.22052505393370983,0.6551297756686417,-0.3845787134116139,-0.43651855202718437,0.3399871308683097,-0.35317733907262716,-0.21954520883308581,-0.21604462791611415,-0.15408684324137564,-0.13290278710374342,0.20353935103663778,-0.06675847079764424,0.30272896939230737,-0.11550020822836823,0.03597464698587911,-0.2930436336857631,-0.21310883386418822,-0.17321026318855953,-0.1287492129276133,-0.28548918359835057,-0.31436273662006436,0.10885928474274373,0.10377900382844367,-0.2711712213034461,0.28911931666613055,-0.2116341641683871,-0.30307754144148485,-0.24975019508815438,0.38072418546682296,-0.06839522142288772,-0.17906786372415623,-0.06854465131622617,-0.2486149726029003,0.15304537510203148,-0.1286395787301384,-0.36652183486600237,-0.16856955031062643,-0.04034513662387044,-0.06336544454094238,-0.22384652227085491,-0.03655090875663519,-0.2659676527180822,-0.11765608723206432,-0.16141101108289047,0.12481232105860725,-0.21408762915791174,-0.3531682447937613,-0.13492117189828595,0.11720512033987122,-0.4023147785003589,-0.07931333418111114,-0.27616590786576545,-0.13125187625710397,-0.3701421388662349,-0.5588940277903153,-0.1845425059504616,-0.08180314486238255,-0.1204750248865935,-0.0019377776293723434,-0.011364415561421148,-0.3484972931721225,-0.40762203455967716,-0.32028830699007843,0.18967570614911,0.06813498184385933,-0.4057047775262244,-0.26107099022432356,0.13579332140576408,-0.3852656411971816,0.22519208710518912,-0.4734163357427364,-0.3877823316997992,-0.31157919231069253,-0.18161569717303533,0.005254101034601298,-1.5741917340368334e-08,2.1509933871480672e-05,0.00021991355193423626,0.006945444506314185]\n",
      "Intercept: -1.538120537252658\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol='label',\n",
    "                       featuresCol='features',\n",
    "                       maxIter=10,\n",
    "                       regParam=0.3)\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "print('Coefficients: ' + str(linearModel.coefficients))\n",
    "print('Intercept: ' + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.90738310397271...|\n",
      "|  0.0|       0.0|[0.92210643707431...|\n",
      "|  0.0|       0.0|[0.91653112086434...|\n",
      "|  0.0|       0.0|[0.92165294477832...|\n",
      "|  0.0|       0.0|[0.63014613036744...|\n",
      "|  0.0|       0.0|[0.64568980001892...|\n",
      "|  0.0|       0.0|[0.65019152944581...|\n",
      "|  0.0|       0.0|[0.73140740027626...|\n",
      "|  0.0|       0.0|[0.91872221474052...|\n",
      "|  0.0|       0.0|[0.83200245622189...|\n",
      "|  0.0|       0.0|[0.83100080914289...|\n",
      "|  0.0|       0.0|[0.83186483418696...|\n",
      "|  0.0|       0.0|[0.54057652000698...|\n",
      "|  0.0|       0.0|[0.64758660638965...|\n",
      "|  0.0|       0.0|[0.65490647855304...|\n",
      "|  0.0|       0.0|[0.57995884653068...|\n",
      "|  0.0|       0.0|[0.86698614738953...|\n",
      "|  0.0|       0.0|[0.79980152459084...|\n",
      "|  0.0|       0.0|[0.84109588766540...|\n",
      "|  0.0|       0.0|[0.51846565288158...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = prediction.select('label', 'prediction', 'probability')\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = prediction.select('label', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        7479|\n",
      "|  1.0|        2387|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('label').agg({'label':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_m(model):\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print('Model accuracy: {:.3f%}' + acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-b051c1c07bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy: {:.3f%}'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "acc = cm.filter(cm.label == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 81.928%\n"
     ]
    }
   ],
   "source": [
    "print('Model accuracy: {:.3f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8918109654105872\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n",
    "print(evaluator.evaluate(prediction))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.5]).build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to train mode: 1187.530\n"
     ]
    }
   ],
   "source": [
    "from time import * \n",
    "t0 = time()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds = 5)\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "t1 = time()\n",
    "elapse = t1-t0\n",
    "print('time to train mode: {:.3f}'.format(elapse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = cvModel.transform(test_data)\n",
    "cm1 = prediction1.select('label', 'prediction')\n",
    "acc1 = cm1.filter(cm1.label == cm1.prediction).count() / cm1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'84.7659%'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:.4f}%'.format(acc1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_eee1a4c06d9e', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='maxIter', doc='maximum number of iterations (>= 0)'): 10,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='regParam', doc='regularization parameter (>= 0)'): 0.01,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_eee1a4c06d9e', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.guru99.com/pyspark-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
